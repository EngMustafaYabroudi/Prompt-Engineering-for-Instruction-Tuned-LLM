{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55e13df9",
   "metadata": {
    "papermill": {
     "duration": 0.010733,
     "end_time": "2024-01-23T08:47:42.081780",
     "exception": false,
     "start_time": "2024-01-23T08:47:42.071047",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <center style=\"font-family: consolas; font-size: 32px; font-weight: bold;\">  Prompt Engineering for Instruction-Tuned LLM: Text Summarization & Information Retrieval </center>\n",
    "***\n",
    "\n",
    "\n",
    "# \n",
    "\n",
    "\n",
    "LLMs offer a revolutionary approach by enabling the execution of various tasks with a single prompt, streamlining the traditional workflow that involves developing and deploying separate models for distinct objectives.\n",
    "\n",
    "Through practical examples, the article illustrates the efficiency of LLMs in tasks such as sentiment analysis of product reviews, identification of emotions, and extraction of valuable information like product and company names from customer reviews. \n",
    "\n",
    "The versatility of LLMs is further demonstrated as they seamlessly perform multiple tasks concurrently through unified prompts. The article also delves into more complex natural language processing tasks, including topic inference and indexing topics for news articles. \n",
    "\n",
    "Overall, the transformative impact of prompt engineering with LLMs is highlighted, presenting an efficient and powerful tool for both seasoned machine learning developers and those entering the field.\n",
    "\n",
    "\n",
    "#### <a id=\"top\"></a>\n",
    "\n",
    "\n",
    "# <div style=\"box-shadow: rgb(60, 121, 245) 0px 0px 0px 3px inset, rgb(255, 255, 255) 10px -10px 0px -3px, rgb(31, 193, 27) 10px -10px, rgb(255, 255, 255) 20px -20px 0px -3px, rgb(255, 217, 19) 20px -20px, rgb(255, 255, 255) 30px -30px 0px -3px, rgb(255, 156, 85) 30px -30px, rgb(255, 255, 255) 40px -40px 0px -3px, rgb(255, 85, 85) 40px -40px; padding:20px; margin-right: 40px; font-size:30px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(60, 121, 245);\"><b>Table of contents</b></div>\n",
    "\n",
    "<div style=\"background-color: rgba(60, 121, 245, 0.03); padding:30px; font-size:15px; font-family: consolas;\">\n",
    "<ul>\n",
    "    <li><a href=\"#1\" target=\"_self\" rel=\" noreferrer nofollow\">1. Getting Started & Setting Working Environment </a> </li>\n",
    "    <li><a href=\"#2\" target=\"_self\" rel=\" noreferrer nofollow\">2. Sentiment Analysis of Product Review </a></li>\n",
    "    <li><a href=\"#3\" target=\"_self\" rel=\" noreferrer nofollow\">3.  Identify Emotions </a></li> \n",
    "    <li><a href=\"#4\" target=\"_self\" rel=\" noreferrer nofollow\">4. Doing Multiple Tasks at Once </a></li>\n",
    "        <li><a href=\"#5\" target=\"_self\" rel=\" noreferrer nofollow\">5. Extract Product & Company Names From Customer Reviews </a></li>\n",
    "            <li><a href=\"#6\" target=\"_self\" rel=\" noreferrer nofollow\">6. Topics Inferring </a></li>\n",
    "            <li><a href=\"#7\" target=\"_self\" rel=\" noreferrer nofollow\">7. Make a News Alert for Certain Topics </a></li>\n",
    "            <li><a href=\"#8\" target=\"_self\" rel=\" noreferrer nofollow\">8. Conclusion </a></li>\n",
    "                <li><a href=\"#9\" target=\"_self\" rel=\" noreferrer nofollow\">9. References </a></li>\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6886a360",
   "metadata": {
    "papermill": {
     "duration": 0.010374,
     "end_time": "2024-01-23T08:47:42.102599",
     "exception": false,
     "start_time": "2024-01-23T08:47:42.092225",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"1\"></a>\n",
    "# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 1. Getting Started & Setting Working Environment </b></div>\n",
    "\n",
    "\n",
    "In the traditional machine learning workflow, you’d have to collect the label data set, train a model, figure out how to deploy the model somewhere in the cloud, and make inferences. This could work pretty well, but it was a lot of work to go through that process. Also for every task, such as sentiment, extracting names, and classifying emotions, you will have to train and deploy a separate model. \n",
    "\n",
    "One of the nice things about the large language models is that you can write a prompt for many tasks like these and have it start generating results immediately. This gives tremendous speed in terms of application development. And you can also just use one model, one API to do many different tasks rather than needing to figure out how to train and deploy a lot of different models.\n",
    "\n",
    "Let's initialize our working environment as usual by importing the OpenAI package and then define the API key from the local .env file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06672463",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T08:47:42.125994Z",
     "iopub.status.busy": "2024-01-23T08:47:42.125064Z",
     "iopub.status.idle": "2024-01-23T08:48:17.104769Z",
     "shell.execute_reply": "2024-01-23T08:48:17.103201Z"
    },
    "papermill": {
     "duration": 34.995403,
     "end_time": "2024-01-23T08:48:17.108658",
     "exception": false,
     "start_time": "2024-01-23T08:47:42.113255",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\r\n",
      "  Obtaining dependency information for openai from https://files.pythonhosted.org/packages/5d/12/d445f6b7309600bcd7f895495603d898a31ee35345094f8e16fd9b5e8718/openai-1.9.0-py3-none-any.whl.metadata\r\n",
      "  Downloading openai-1.9.0-py3-none-any.whl.metadata (18 kB)\r\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai) (3.7.1)\r\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai) (1.9.0)\r\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\r\n",
      "  Obtaining dependency information for httpx<1,>=0.23.0 from https://files.pythonhosted.org/packages/39/9b/4937d841aee9c2c8102d9a4eeb800c7dad25386caabb4a1bf5010df81a57/httpx-0.26.0-py3-none-any.whl.metadata\r\n",
      "  Downloading httpx-0.26.0-py3-none-any.whl.metadata (7.6 kB)\r\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from openai) (1.10.12)\r\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai) (1.3.0)\r\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.10/site-packages (from openai) (4.66.1)\r\n",
      "Collecting typing-extensions<5,>=4.7 (from openai)\r\n",
      "  Obtaining dependency information for typing-extensions<5,>=4.7 from https://files.pythonhosted.org/packages/b7/f4/6a90020cd2d93349b442bfcb657d0dc91eee65491600b2cb1d388bc98e6b/typing_extensions-4.9.0-py3-none-any.whl.metadata\r\n",
      "  Downloading typing_extensions-4.9.0-py3-none-any.whl.metadata (3.0 kB)\r\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.4)\r\n",
      "Requirement already satisfied: exceptiongroup in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.1.3)\r\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2023.11.17)\r\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\r\n",
      "  Obtaining dependency information for httpcore==1.* from https://files.pythonhosted.org/packages/56/ba/78b0a99c4da0ff8b0f59defa2f13ca4668189b134bd9840b6202a93d9a0f/httpcore-1.0.2-py3-none-any.whl.metadata\r\n",
      "  Downloading httpcore-1.0.2-py3-none-any.whl.metadata (20 kB)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\r\n",
      "Downloading openai-1.9.0-py3-none-any.whl (223 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m223.4/223.4 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading httpx-0.26.0-py3-none-any.whl (75 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.9/75.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading httpcore-1.0.2-py3-none-any.whl (76 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading typing_extensions-4.9.0-py3-none-any.whl (32 kB)\r\n",
      "Installing collected packages: typing-extensions, httpcore, httpx, openai\r\n",
      "  Attempting uninstall: typing-extensions\r\n",
      "    Found existing installation: typing_extensions 4.5.0\r\n",
      "    Uninstalling typing_extensions-4.5.0:\r\n",
      "      Successfully uninstalled typing_extensions-4.5.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 14.0.2 which is incompatible.\r\n",
      "jupyterlab 4.0.10 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\n",
      "jupyterlab-lsp 5.0.1 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\n",
      "pymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.24.3 which is incompatible.\r\n",
      "pymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.4 which is incompatible.\r\n",
      "tensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.9.0 which is incompatible.\r\n",
      "tensorflow-probability 0.21.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\r\n",
      "tensorflowjs 4.15.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\r\n",
      "ydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.3 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed httpcore-1.0.2 httpx-0.26.0 openai-1.9.0 typing-extensions-4.7.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install openai\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import os\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "openai.api_key = user_secrets.get_secret(\"openai_api\")\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=openai.api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbddac57",
   "metadata": {
    "papermill": {
     "duration": 0.012372,
     "end_time": "2024-01-23T08:48:17.135127",
     "exception": false,
     "start_time": "2024-01-23T08:48:17.122755",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now let's define a function that will take the prompt and return the response from the LLM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34aa1734",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T08:48:17.162937Z",
     "iopub.status.busy": "2024-01-23T08:48:17.162417Z",
     "iopub.status.idle": "2024-01-23T08:48:17.169570Z",
     "shell.execute_reply": "2024-01-23T08:48:17.168322Z"
    },
    "papermill": {
     "duration": 0.024876,
     "end_time": "2024-01-23T08:48:17.172402",
     "exception": false,
     "start_time": "2024-01-23T08:48:17.147526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488d4538",
   "metadata": {
    "papermill": {
     "duration": 0.014494,
     "end_time": "2024-01-23T08:48:17.199380",
     "exception": false,
     "start_time": "2024-01-23T08:48:17.184886",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"2\"></a>\n",
    "# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 2. Sentiment Analysis of Product Review </b></div>\n",
    "\n",
    "\n",
    "Sentiment analysis is a natural language processing (NLP) technique used to determine the sentiment expressed in a piece of text. When it comes to product reviews, sentiment analysis can be valuable for businesses to understand customer opinions and feedback. In this article, we will use a review about a product which will be a lamp. Here is the review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7dbf0746",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T08:48:17.227091Z",
     "iopub.status.busy": "2024-01-23T08:48:17.226204Z",
     "iopub.status.idle": "2024-01-23T08:48:17.233251Z",
     "shell.execute_reply": "2024-01-23T08:48:17.231968Z"
    },
    "papermill": {
     "duration": 0.024338,
     "end_time": "2024-01-23T08:48:17.236434",
     "exception": false,
     "start_time": "2024-01-23T08:48:17.212096",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lamp_review = \"\"\"\n",
    "Needed a nice lamp for my bedroom, and this one had \\\n",
    "additional storage and not too high of a price point. \\\n",
    "Got it fast.  The string to our lamp broke during the \\\n",
    "transit and the company happily sent over a new one. \\\n",
    "Came within a few days as well. It was easy to put \\\n",
    "together.  I had a missing part, so I contacted their \\\n",
    "support and they very quickly got me the missing piece! \\\n",
    "Lumina seems to me to be a great company that cares \\\n",
    "about their customers and products!!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88078e0c",
   "metadata": {
    "papermill": {
     "duration": 0.012622,
     "end_time": "2024-01-23T08:48:17.261869",
     "exception": false,
     "start_time": "2024-01-23T08:48:17.249247",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's write a prompt to classify the sentiment of this review. I want the system to tell me what is the sentiment. We will ask it “What is the sentiment of the following product review” with the usual delimiter and we will then run that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f71bceff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T08:48:17.289331Z",
     "iopub.status.busy": "2024-01-23T08:48:17.288755Z",
     "iopub.status.idle": "2024-01-23T08:48:18.585110Z",
     "shell.execute_reply": "2024-01-23T08:48:18.583195Z"
    },
    "papermill": {
     "duration": 1.313448,
     "end_time": "2024-01-23T08:48:18.588219",
     "exception": false,
     "start_time": "2024-01-23T08:48:17.274771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentiment of the product review is positive.\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "What is the sentiment of the following product review, \n",
    "which is delimited with triple backticks?\n",
    "\n",
    "Review text: '''{lamp_review}'''\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b70c7c1",
   "metadata": {
    "papermill": {
     "duration": 0.011986,
     "end_time": "2024-01-23T08:48:18.613827",
     "exception": false,
     "start_time": "2024-01-23T08:48:18.601841",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "It says, **The sentiment of the product review is positive.**, which actually, seems pretty right.\n",
    "\n",
    "If you want to give a more concise response to make it easier for post-processing, I can take this prompt and add another instruction to give you answers to a single word, either positive or negative. So it just prints out positively like this, which makes it easier for a piece of text to take this output process it, and do something with it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e61d63e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T08:48:18.640722Z",
     "iopub.status.busy": "2024-01-23T08:48:18.640271Z",
     "iopub.status.idle": "2024-01-23T08:48:19.174393Z",
     "shell.execute_reply": "2024-01-23T08:48:19.173053Z"
    },
    "papermill": {
     "duration": 0.551339,
     "end_time": "2024-01-23T08:48:19.177398",
     "exception": false,
     "start_time": "2024-01-23T08:48:18.626059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "What is the sentiment of the following product review, \n",
    "which is delimited with triple backticks?\n",
    "\n",
    "Give your answer as a single word, either \"positive\" \\\n",
    "or \"negative\".\n",
    "\n",
    "Review text: '''{lamp_review}'''\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaf4f87",
   "metadata": {
    "papermill": {
     "duration": 0.013328,
     "end_time": "2024-01-23T08:48:19.203908",
     "exception": false,
     "start_time": "2024-01-23T08:48:19.190580",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"3\"></a>\n",
    "# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 3. Identify Emotions </b></div>\n",
    "\n",
    "Let’s take another use case where we can use prompt engineering to do a complex sentiment task which is to identify emotions. So, large language models are pretty good at extracting specific things out of a piece of text. In this case, we’re expressing the emotions and this could be useful for understanding how your customers think about a particular product. For a lot of customer support organizations, it’s important to understand if a particular user is extremely upset. Here is the prompt to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69843462",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T08:48:19.231826Z",
     "iopub.status.busy": "2024-01-23T08:48:19.230702Z",
     "iopub.status.idle": "2024-01-23T08:48:20.630454Z",
     "shell.execute_reply": "2024-01-23T08:48:20.627943Z"
    },
    "papermill": {
     "duration": 1.418642,
     "end_time": "2024-01-23T08:48:20.635323",
     "exception": false,
     "start_time": "2024-01-23T08:48:19.216681",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "satisfied, pleased, grateful, impressed, happy\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Identify a list of emotions that the writer of the \\\n",
    "following review is expressing. Include no more than \\\n",
    "five items in the list. Format your answer as a list of \\\n",
    "lower-case words separated by commas.\n",
    "\n",
    "Review text: '''{lamp_review}'''\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4815ed",
   "metadata": {
    "papermill": {
     "duration": 0.016553,
     "end_time": "2024-01-23T08:48:20.666755",
     "exception": false,
     "start_time": "2024-01-23T08:48:20.650202",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "You might have a different classification problem such as identifying a c certain emotion. So I would like to know whether the customer is angry or not. If someone is really angry, it might merit paying extra attention to have a customer review, to have customer support or customer success, to reach out to figure out what’s going on and make things right for the customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9962064",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T08:48:20.702243Z",
     "iopub.status.busy": "2024-01-23T08:48:20.700926Z",
     "iopub.status.idle": "2024-01-23T08:48:21.420871Z",
     "shell.execute_reply": "2024-01-23T08:48:21.419800Z"
    },
    "papermill": {
     "duration": 0.741643,
     "end_time": "2024-01-23T08:48:21.423985",
     "exception": false,
     "start_time": "2024-01-23T08:48:20.682342",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Is the writer of the following review expressing anger?\\\n",
    "The review is delimited with triple backticks. \\\n",
    "Give your answer as either yes or no.\n",
    "\n",
    "Review text: '''{lamp_review}'''\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcf4921",
   "metadata": {
    "papermill": {
     "duration": 0.013393,
     "end_time": "2024-01-23T08:48:21.450187",
     "exception": false,
     "start_time": "2024-01-23T08:48:21.436794",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this case, the customer is not angry. Notice that with supervised learning, if I had wanted to build all of these classifiers, there’s no way I would have been able to do this with supervised learning in just a few minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ff04e5",
   "metadata": {
    "papermill": {
     "duration": 0.013042,
     "end_time": "2024-01-23T08:48:21.475949",
     "exception": false,
     "start_time": "2024-01-23T08:48:21.462907",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"4\"></a>\n",
    "# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 4. Extract Product & Company Names From Customer Reviews </b></div>\n",
    "\n",
    "\n",
    "Information extraction is a part of NLP that relates to taking a piece of text and extracting certain things that you want to know from the text. So in this prompt, I will ask the LLM to identify the item purchased and the name of the company that made the item. \n",
    "\n",
    "If you are trying to summarize many reviews from an online shopping e-commerce website, it might be useful for your large collection of reviews to figure out what were the items, who made the item, and the positive and negative sentiments, to track trends about positive or negative sentiment for specific items or for specific manufacturers. In this example, I’m going to ask it to format your response as a JSON object with “**Item**” and “**Brand**” as the keys.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaaf7f75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T08:48:21.504334Z",
     "iopub.status.busy": "2024-01-23T08:48:21.503329Z",
     "iopub.status.idle": "2024-01-23T08:48:23.509225Z",
     "shell.execute_reply": "2024-01-23T08:48:23.507798Z"
    },
    "papermill": {
     "duration": 2.024469,
     "end_time": "2024-01-23T08:48:23.512942",
     "exception": false,
     "start_time": "2024-01-23T08:48:21.488473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Item\": \"lamp\",\n",
      "  \"Brand\": \"Lumina\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Identify the following items from the review text: \n",
    "- Item purchased by reviewer\n",
    "- Company that made the item\n",
    "\n",
    "The review is delimited with triple backticks. \\\n",
    "Format your response as a JSON object with \\\n",
    "\"Item\" and \"Brand\" as the keys. \n",
    "If the information isn't present, use \"unknown\" \\\n",
    "as the value.\n",
    "Make your response as short as possible.\n",
    "  \n",
    "Review text: '''{lamp_review}'''\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e92a5a",
   "metadata": {
    "papermill": {
     "duration": 0.012474,
     "end_time": "2024-01-23T08:48:23.539125",
     "exception": false,
     "start_time": "2024-01-23T08:48:23.526651",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"5\"></a>\n",
    "# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 5. Doing Multiple Tasks at Once </b></div>\n",
    "\n",
    "\n",
    "\n",
    "In the examples we’ve gone through, we have written prompts to recognize the sentiment, figure out if someone is angry, and then also extract the item and the brand. \n",
    "\n",
    "One way to extract all of this information would be to use three or four prompts and call the “get_completion” function, three times or four times to extract these different views one at a time. But it turns out you can write a single prompt to extract all of this information at the same time.\n",
    "\n",
    "So let’s say “identify the following items, extract sentiment, is the reviewer expressing anger, item purchased, company that made it”. Then I’m going to tell it to format the anger value as a boolean value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31ff992b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T08:48:23.566090Z",
     "iopub.status.busy": "2024-01-23T08:48:23.565556Z",
     "iopub.status.idle": "2024-01-23T08:48:26.388103Z",
     "shell.execute_reply": "2024-01-23T08:48:26.386829Z"
    },
    "papermill": {
     "duration": 2.842555,
     "end_time": "2024-01-23T08:48:26.393840",
     "exception": false,
     "start_time": "2024-01-23T08:48:23.551285",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Sentiment\": \"positive\",\n",
      "  \"Anger\": false,\n",
      "  \"Item\": \"lamp\",\n",
      "  \"Brand\": \"Lumina\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Identify the following items from the review text: \n",
    "- Sentiment (positive or negative)\n",
    "- Is the reviewer expressing anger? (true or false)\n",
    "- Item purchased by reviewer\n",
    "- Company that made the item\n",
    "\n",
    "The review is delimited with triple backticks. \\\n",
    "Format your response as a JSON object with \\\n",
    "\"Sentiment\", \"Anger\", \"Item\" and \"Brand\" as the keys.\n",
    "If the information isn't present, use \"unknown\" \\\n",
    "as the value.\n",
    "Make your response as short as possible.\n",
    "Format the Anger value as a boolean.\n",
    "\n",
    "Review text: '''{lamp_review}'''\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cb2b30",
   "metadata": {
    "papermill": {
     "duration": 0.012392,
     "end_time": "2024-01-23T08:48:26.419371",
     "exception": false,
     "start_time": "2024-01-23T08:48:26.406979",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"6\"></a>\n",
    "# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 6. Topics Inferring </b></div>\n",
    "\n",
    "Another complex NLP task that you can do using is topic inferring. Given a long piece of text, you know, what is this piece of text about? What are the topics? \n",
    "\n",
    "Here’s a fictitious newspaper article about how government workers feel about the agency they work for. So, the recent survey conducted by the government.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "161f2144",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T08:48:26.448308Z",
     "iopub.status.busy": "2024-01-23T08:48:26.447434Z",
     "iopub.status.idle": "2024-01-23T08:48:26.453566Z",
     "shell.execute_reply": "2024-01-23T08:48:26.452552Z"
    },
    "papermill": {
     "duration": 0.023878,
     "end_time": "2024-01-23T08:48:26.456211",
     "exception": false,
     "start_time": "2024-01-23T08:48:26.432333",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "story = \"\"\"\n",
    "In a recent survey conducted by the government, \n",
    "public sector employees were asked to rate their level \n",
    "of satisfaction with the department they work at. \n",
    "The results revealed that NASA was the most popular \n",
    "department with a satisfaction rating of 95%.\n",
    "\n",
    "One NASA employee, John Smith, commented on the findings, \n",
    "stating, \"I'm not surprised that NASA came out on top. \n",
    "It's a great place to work with amazing people and \n",
    "incredible opportunities. I'm proud to be a part of \n",
    "such an innovative organization.\"\n",
    "\n",
    "The results were also welcomed by NASA's management team, \n",
    "with Director Tom Johnson stating, \"We are thrilled to \n",
    "hear that our employees are satisfied with their work at NASA. \n",
    "We have a talented and dedicated team who work tirelessly \n",
    "to achieve our goals, and it's fantastic to see that their \n",
    "hard work is paying off.\"\n",
    "\n",
    "The survey also revealed that the \n",
    "Social Security Administration had the lowest satisfaction \n",
    "rating, with only 45% of employees indicating they were \n",
    "satisfied with their job. The government has pledged to \n",
    "address the concerns raised by employees in the survey and \n",
    "work towards improving job satisfaction across all departments.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e86e32",
   "metadata": {
    "papermill": {
     "duration": 0.012525,
     "end_time": "2024-01-23T08:48:26.481896",
     "exception": false,
     "start_time": "2024-01-23T08:48:26.469371",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "So, given an article like this, we can ask it, with this prompt, to determine five topics that are being discussed in the following text. Let’s make each item one or two words long, for my response, in a comma-separated list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa64d99d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T08:48:26.510296Z",
     "iopub.status.busy": "2024-01-23T08:48:26.509370Z",
     "iopub.status.idle": "2024-01-23T08:48:28.997675Z",
     "shell.execute_reply": "2024-01-23T08:48:28.996277Z"
    },
    "papermill": {
     "duration": 2.50588,
     "end_time": "2024-01-23T08:48:29.000500",
     "exception": false,
     "start_time": "2024-01-23T08:48:26.494620",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Government survey\n",
      "2. Department satisfaction rating\n",
      "3. NASA\n",
      "4. Social Security Administration\n",
      "5. Job satisfaction improvement\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Determine five topics that are being discussed in the \\\n",
    "following text, which is delimited by triple backticks.\n",
    "\n",
    "Make each item one or two words long. \n",
    "\n",
    "Format your response as a list of items separated by commas.\n",
    "\n",
    "Text sample: '''{story}'''\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bb03ab",
   "metadata": {
    "papermill": {
     "duration": 0.012281,
     "end_time": "2024-01-23T08:48:29.025311",
     "exception": false,
     "start_time": "2024-01-23T08:48:29.013030",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "It’s about a government survey, it’s about job satisfaction, it’s about NASA, and so on. So, overall, I think, a pretty nice extraction of a list of topics. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8df437a",
   "metadata": {
    "papermill": {
     "duration": 0.012463,
     "end_time": "2024-01-23T08:48:29.050853",
     "exception": false,
     "start_time": "2024-01-23T08:48:29.038390",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"7\"></a>\n",
    "# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 7. Make Alert for Certain Topics\n",
    " </b></div>\n",
    "\n",
    " \n",
    " \n",
    " If you have a collection of articles and extract topics, you can then also use a large language model to help you index into different topics. So, let me use a slightly different topic list.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2070eb21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T08:48:29.080446Z",
     "iopub.status.busy": "2024-01-23T08:48:29.079716Z",
     "iopub.status.idle": "2024-01-23T08:48:29.085074Z",
     "shell.execute_reply": "2024-01-23T08:48:29.084021Z"
    },
    "papermill": {
     "duration": 0.023901,
     "end_time": "2024-01-23T08:48:29.087815",
     "exception": false,
     "start_time": "2024-01-23T08:48:29.063914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "topic_list = [\n",
    "    \"nasa\", \"local government\", \"engineering\", \n",
    "    \"employee satisfaction\", \"federal government\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0299e197",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-23T08:48:29.115261Z",
     "iopub.status.busy": "2024-01-23T08:48:29.114794Z",
     "iopub.status.idle": "2024-01-23T08:48:30.559575Z",
     "shell.execute_reply": "2024-01-23T08:48:30.558379Z"
    },
    "papermill": {
     "duration": 1.461758,
     "end_time": "2024-01-23T08:48:30.562299",
     "exception": false,
     "start_time": "2024-01-23T08:48:29.100541",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "prompt = f\"\"\"\n",
    "Determine whether each item in the following list of \\\n",
    "topics is a topic in the text below, which\n",
    "is delimited with triple backticks.\n",
    "\n",
    "Give your answer as list with 0 or 1 for each topic.\\\n",
    "\n",
    "List of topics: {\", \".join(topic_list)}\n",
    "\n",
    "Text sample: '''{story}'''\n",
    "\"\"\"\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8295e997",
   "metadata": {
    "papermill": {
     "duration": 0.103861,
     "end_time": "2024-01-23T08:48:30.678621",
     "exception": false,
     "start_time": "2024-01-23T08:48:30.574760",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In machine learning, this is sometimes called a “Zero-Shot Learning Algorithm”, because we didn’t give it any training data that was labeled, so that’s Zero-Shot. With just a prompt, it was able to determine which of these topics were covered in that news article. So, if you want to generate a news alert, say, so that process news, and I like a lot of work that NASA does. So, if you want to build a system that can take this, put this information into a dictionary, and whenever NASA news pops up, print “ALERT: New NASA story!”, they can use this to very quickly take any article, figure out what topics it is about, and if the topic includes NASA, have it print out “ALERT: New NASA story!”. \n",
    "\n",
    "This prompt that I use up here isn’t very robust. If I wanted a production system, I would probably have it output the answer in JSON format, rather than as a list, because the output of the large language model can be a little bit inconsistent. So, this is a pretty brittle piece of code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf6168b",
   "metadata": {
    "papermill": {
     "duration": 0.012415,
     "end_time": "2024-01-23T08:48:30.703716",
     "exception": false,
     "start_time": "2024-01-23T08:48:30.691301",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"8\"></a>\n",
    "# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 8. Conclusion </b></div>\n",
    "\n",
    "In conclusion in just a few minutes, you can build multiple systems for making inferences about text that previously would have taken days or even weeks for a skilled machine-learning developer. \n",
    "\n",
    "This is very exciting that both for skilled machine learning developers, as well as for people who are newer to machine learning, you can now use prompting to very quickly build and start making inferences on pretty complicated natural language processing tasks like these."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb342f3",
   "metadata": {
    "papermill": {
     "duration": 0.013147,
     "end_time": "2024-01-23T08:48:30.730718",
     "exception": false,
     "start_time": "2024-01-23T08:48:30.717571",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"9\"></a>\n",
    "# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 9. References  </b></div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e892d3",
   "metadata": {
    "papermill": {
     "duration": 0.012608,
     "end_time": "2024-01-23T08:48:30.756296",
     "exception": false,
     "start_time": "2024-01-23T08:48:30.743688",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30635,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 54.234747,
   "end_time": "2024-01-23T08:48:31.494197",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-01-23T08:47:37.259450",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
