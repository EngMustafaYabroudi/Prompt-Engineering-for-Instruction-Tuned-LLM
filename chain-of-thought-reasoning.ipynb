{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a506c19",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.006642,
     "end_time": "2024-03-17T13:41:07.171032",
     "exception": false,
     "start_time": "2024-03-17T13:41:07.164390",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <center style=\"font-family: consolas; font-size: 32px; font-weight: bold;\">  Prompt Engineering Best Practices: Chain of Thought Reasoning </center>\n",
    "\n",
    "# <center style=\"font-family: consolas; font-size: 25px; font-weight: bold;\">  Prompt Engineering for Instruction-Tuned LLMs </center>\n",
    "***\n",
    "\n",
    "Prompt engineering, a fundamental concept in AI development, involves crafting tailored instructions or queries to guide AI models in generating desired outputs effectively across diverse tasks and scenarios.¬†\n",
    "\n",
    "The notebook introduces the Chain of Thought Reasoning technique, which systematically guides AI models through step-by-step reasoning processes. This approach breaks down complex problems into manageable steps, enabling models to produce more accurate and coherent responses by considering various reasoning paths.\n",
    "\n",
    "This comprehensive notebook covers the rationale behind using Chain of Thought Reasoning, practical examples demonstrating its application, guidelines for prompt structuring, and handling diverse user queries effectively. Additionally, it introduces the Inner Monologue concept for privacy preservation and recommends experimenting with prompt complexity to find the optimal balance between effectiveness and simplicity.\n",
    "\n",
    "By exploring the strategies outlined in the notebook, readers can enhance the accuracy and coherence of AI-generated responses, improve user interactions, and safeguard privacy. Implementing the principles of prompt engineering and Chain of Thought Reasoning enables developers to create more efficient AI models, providing structured and informative interactions while optimizing user experience.\n",
    "\n",
    "#### <a id=\"top\"></a>\n",
    "# <div style=\"box-shadow: rgb(60, 121, 245) 0px 0px 0px 3px inset, rgb(255, 255, 255) 10px -10px 0px -3px, rgb(31, 193, 27) 10px -10px, rgb(255, 255, 255) 20px -20px 0px -3px, rgb(255, 217, 19) 20px -20px, rgb(255, 255, 255) 30px -30px 0px -3px, rgb(255, 156, 85) 30px -30px, rgb(255, 255, 255) 40px -40px 0px -3px, rgb(255, 85, 85) 40px -40px; padding:20px; margin-right: 40px; font-size:30px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(60, 121, 245);\"><b>Table of contents</b></div>\n",
    "\n",
    "<div style=\"background-color: rgba(60, 121, 245, 0.03); padding:30px; font-size:15px; font-family: consolas;\">\n",
    "<ul>\n",
    "    <li><a href=\"#1\" target=\"_self\" rel=\" noreferrer nofollow\">1. Introducing Chain of Thought Reasoning </a> </li>\n",
    "    <li><a href=\"#2\" target=\"_self\" rel=\" noreferrer nofollow\">2. Setting Up Working Environment </a></li>\n",
    "    <li><a href=\"#3\" target=\"_self\" rel=\" noreferrer nofollow\">3. Chain of Thought Reasoning Practical Example </a></li> \n",
    "    <li><a href=\"#4\" target=\"_self\" rel=\" noreferrer nofollow\">4. Inner Monologue Removal </a></li>     \n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "***\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0296bc90",
   "metadata": {
    "papermill": {
     "duration": 0.00583,
     "end_time": "2024-03-17T13:41:07.183270",
     "exception": false,
     "start_time": "2024-03-17T13:41:07.177440",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"1\"></a>\n",
    "# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 1. Introducing Chain of Thought Reasoning </b></div>\n",
    "\n",
    "\n",
    "Sometimes the model needs to reason in detail about a problem before answering a specific question. The model might make reasoning errors by rushing to an incorrect conclusion. Therefore we might need to reframe the query to request a series of relevant reasoning steps before the model provides a final answer.\n",
    "\n",
    "By doing this it can think longer and more methodically about the problem and in general we call the strategy of asking the model to reason about a problem in steps **Chain of Thought reasoning**.\n",
    "For some applications the reasoning process that our model uses to arrive at a final answer would be inappropriate to share with the user for example in tutoring applications we may want to encourage students to work on their answers but a model's reasoning process about the student solution could reveal the answer to the student in a **monologue**.\n",
    "\n",
    "This  is a tactic that we will show by the end of the notebook that can be used to mitigate this and this is just a fancy way of hiding the model's reasoning from the user. The idea of the **inner monologue** is to instruct the model to put parts of the output that are meant to be hidden from the user into a structured format that makes passing them easy then before presenting the output to the user the output is pass\n",
    "ed and only part of the output is made visible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509adb6b",
   "metadata": {
    "papermill": {
     "duration": 0.005691,
     "end_time": "2024-03-17T13:41:07.194966",
     "exception": false,
     "start_time": "2024-03-17T13:41:07.189275",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"2\"></a>\n",
    "# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 2. Setting Up the Working Environment </b></div>\n",
    "\n",
    "\n",
    "Let's initialize our working environment as usual by importing the OpenAI package and then define the API key from the local¬†**.env** file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7ea9462",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-17T13:41:07.209745Z",
     "iopub.status.busy": "2024-03-17T13:41:07.208705Z",
     "iopub.status.idle": "2024-03-17T13:41:25.211959Z",
     "shell.execute_reply": "2024-03-17T13:41:25.210777Z"
    },
    "papermill": {
     "duration": 18.013856,
     "end_time": "2024-03-17T13:41:25.214887",
     "exception": false,
     "start_time": "2024-03-17T13:41:07.201031",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\r\n",
      "  Downloading openai-1.14.1-py3-none-any.whl.metadata (18 kB)\r\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai) (4.2.0)\r\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai) (1.9.0)\r\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from openai) (0.27.0)\r\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from openai) (2.5.3)\r\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai) (1.3.0)\r\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.10/site-packages (from openai) (4.66.1)\r\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/lib/python3.10/site-packages (from openai) (4.9.0)\r\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.6)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\r\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\r\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.4)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.14.6)\r\n",
      "Downloading openai-1.14.1-py3-none-any.whl (257 kB)\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m257.5/257.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: openai\r\n",
      "Successfully installed openai-1.14.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install openai\n",
    "\n",
    "from openai import OpenAI\n",
    "import openai\n",
    "import os\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "openai.api_key = user_secrets.get_secret(\"openai_api\")\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=openai.api_key,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43f472f",
   "metadata": {
    "papermill": {
     "duration": 0.006856,
     "end_time": "2024-03-17T13:41:25.229038",
     "exception": false,
     "start_time": "2024-03-17T13:41:25.222182",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now let's define a function that will take the prompt and return the response from the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "993bc6ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-17T13:41:25.245320Z",
     "iopub.status.busy": "2024-03-17T13:41:25.244778Z",
     "iopub.status.idle": "2024-03-17T13:41:25.251591Z",
     "shell.execute_reply": "2024-03-17T13:41:25.250451Z"
    },
    "papermill": {
     "duration": 0.017977,
     "end_time": "2024-03-17T13:41:25.254125",
     "exception": false,
     "start_time": "2024-03-17T13:41:25.236148",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_completion_from_messages(messages, \n",
    "                                 model=\"gpt-3.5-turbo\", \n",
    "                                 temperature=0, max_tokens=500):\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, \n",
    "        max_tokens=max_tokens, \n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db487b6",
   "metadata": {
    "papermill": {
     "duration": 0.006789,
     "end_time": "2024-03-17T13:41:25.268071",
     "exception": false,
     "start_time": "2024-03-17T13:41:25.261282",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"3\"></a>\n",
    "# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 3. Chain of Thought Reasoning Practical Example </b></div>\n",
    "\n",
    "\n",
    "Let's take a scenario in which we want the model to classify a customer query into a primary and secondary category and based on this  classification we might want to give the model different instructions. So if the product information category is in the next instructions we want to include information about this product.\n",
    "Let's dive into this example and we will start with the delimiters that we will be using:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4afc37c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-17T13:41:25.283891Z",
     "iopub.status.busy": "2024-03-17T13:41:25.283485Z",
     "iopub.status.idle": "2024-03-17T13:41:25.288363Z",
     "shell.execute_reply": "2024-03-17T13:41:25.287288Z"
    },
    "papermill": {
     "duration": 0.01536,
     "end_time": "2024-03-17T13:41:25.290620",
     "exception": false,
     "start_time": "2024-03-17T13:41:25.275260",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "delimiter = \"####\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e55c61",
   "metadata": {
    "papermill": {
     "duration": 0.006819,
     "end_time": "2024-03-17T13:41:25.304505",
     "exception": false,
     "start_time": "2024-03-17T13:41:25.297686",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's define  the  system message in which we will ask the model to reason about the answer before coming to its conclusion. So the instruction is to follow these steps to answer the customer queries the customer query will be delimited with four hashtags or delimiter.\n",
    "\n",
    "Next, we will split the instructions into five steps. The first step is to decide whether the user is asking a question about a specific product or product category doesn't count.\n",
    "Step two identify whether the products the user is asking about are in the following list and we will include a list of available products. Here we have five available products that are all varieties of laptops and these are all generated by GPT 4.\n",
    "\n",
    "Then we will go to step three if the user is asking about specific products that are available in the above list. We will ask the LLM to list above any assumptions that the user is making in their message for example that laptop X is bigger than laptop Y or that laptop Z has a two-year warranty.\n",
    "\n",
    "Step four is if the user made any assumptions figure out whether the assumption is true based on your product information\n",
    "\n",
    "In step five we will ask the LLM to politely correct the customer's incorrect assumptions if applicable only mention or reference products in the list of the five available products as these are the only five products that the store sells and answer the customer in a friendly tone.\n",
    "\n",
    "Finally, we will ask the model to use the following format step that will make it easier for us later to get just this response to the customer. Now let's put these whole steps together to create the final prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4209946",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-17T13:41:25.320988Z",
     "iopub.status.busy": "2024-03-17T13:41:25.320583Z",
     "iopub.status.idle": "2024-03-17T13:41:25.329750Z",
     "shell.execute_reply": "2024-03-17T13:41:25.328609Z"
    },
    "papermill": {
     "duration": 0.020505,
     "end_time": "2024-03-17T13:41:25.332284",
     "exception": false,
     "start_time": "2024-03-17T13:41:25.311779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "delimiter = \"####\"\n",
    "system_message = f\"\"\"\n",
    "Follow these steps to answer the customer queries.\n",
    "The customer query will be delimited with four hashtags,\\\n",
    "i.e. {delimiter}. \n",
    "\n",
    "Step 1:{delimiter} First decide whether the user is \\\n",
    "asking a question about a specific product or products. \\\n",
    "Product cateogry doesn't count. \n",
    "\n",
    "Step 2:{delimiter} If the user is asking about \\\n",
    "specific products, identify whether \\\n",
    "the products are in the following list.\n",
    "All available products: \n",
    "1. Product: TechPro Ultrabook\n",
    "   Category: Computers and Laptops\n",
    "   Brand: TechPro\n",
    "   Model Number: TP-UB100\n",
    "   Warranty: 1 year\n",
    "   Rating: 4.5\n",
    "   Features: 13.3-inch display, 8GB RAM, 256GB SSD, Intel Core i5 processor\n",
    "   Description: A sleek and lightweight ultrabook for everyday use.\n",
    "   Price: $799.99\n",
    "\n",
    "2. Product: BlueWave Gaming Laptop\n",
    "   Category: Computers and Laptops\n",
    "   Brand: BlueWave\n",
    "   Model Number: BW-GL200\n",
    "   Warranty: 2 years\n",
    "   Rating: 4.7\n",
    "   Features: 15.6-inch display, 16GB RAM, 512GB SSD, NVIDIA GeForce RTX 3060\n",
    "   Description: A high-performance gaming laptop for an immersive experience.\n",
    "   Price: $1199.99\n",
    "\n",
    "3. Product: PowerLite Convertible\n",
    "   Category: Computers and Laptops\n",
    "   Brand: PowerLite\n",
    "   Model Number: PL-CV300\n",
    "   Warranty: 1 year\n",
    "   Rating: 4.3\n",
    "   Features: 14-inch touchscreen, 8GB RAM, 256GB SSD, 360-degree hinge\n",
    "   Description: A versatile convertible laptop with a responsive touchscreen.\n",
    "   Price: $699.99\n",
    "\n",
    "4. Product: TechPro Desktop\n",
    "   Category: Computers and Laptops\n",
    "   Brand: TechPro\n",
    "   Model Number: TP-DT500\n",
    "   Warranty: 1 year\n",
    "   Rating: 4.4\n",
    "   Features: Intel Core i7 processor, 16GB RAM, 1TB HDD, NVIDIA GeForce GTX 1660\n",
    "   Description: A powerful desktop computer for work and play.\n",
    "   Price: $999.99\n",
    "\n",
    "5. Product: BlueWave Chromebook\n",
    "   Category: Computers and Laptops\n",
    "   Brand: BlueWave\n",
    "   Model Number: BW-CB100\n",
    "   Warranty: 1 year\n",
    "   Rating: 4.1\n",
    "   Features: 11.6-inch display, 4GB RAM, 32GB eMMC, Chrome OS\n",
    "   Description: A compact and affordable Chromebook for everyday tasks.\n",
    "   Price: $249.99\n",
    "\n",
    "Step 3:{delimiter} If the message contains products \\\n",
    "in the list above, list any assumptions that the \\\n",
    "user is making in their \\\n",
    "message e.g. that Laptop X is bigger than \\\n",
    "Laptop Y, or that Laptop Z has a 2 year warranty.\n",
    "\n",
    "Step 4:{delimiter}: If the user made any assumptions, \\\n",
    "figure out whether the assumption is true based on your \\\n",
    "product information. \n",
    "\n",
    "Step 5:{delimiter}: First, politely correct the \\\n",
    "customer's incorrect assumptions if applicable. \\\n",
    "Only mention or reference products in the list of \\\n",
    "5 available products, as these are the only 5 \\\n",
    "products that the store sells. \\\n",
    "Answer the customer in a friendly tone.\n",
    "\n",
    "Use the following format:\n",
    "Step 1:{delimiter} <step 1 reasoning>\n",
    "Step 2:{delimiter} <step 2 reasoning>\n",
    "Step 3:{delimiter} <step 3 reasoning>\n",
    "Step 4:{delimiter} <step 4 reasoning>\n",
    "Response to user:{delimiter} <response to customer>\n",
    "\n",
    "Make sure to include {delimiter} to separate every step.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4768b0",
   "metadata": {
    "papermill": {
     "duration": 0.006906,
     "end_time": "2024-03-17T13:41:25.346337",
     "exception": false,
     "start_time": "2024-03-17T13:41:25.339431",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now let's try an example in which the input will be \"**By how much is the Blue Wave Chromebook more expensive than the Tech Pro desktop**\" So let's take a look at these two products the Blue Wave Chromebook is 249.99 and the Tech Pro desktop is 999.99.¬†\n",
    "\n",
    "This is not true and it will be interesting to see how the model will handle this user request. We will format it into our messages array and we'll get our response and then we'll print it. We are hoping that the model takes all of these different steps and realizes that the user has made an incorrect assumption and then follows the final step to politely correct the user.\n",
    "\n",
    "So within this one prompt, we've maintained several different complex states that the system could be in so you know at any given point there could be a different output from the previous step and we would want to do something different for example if the user hadn't made any assumptions in step three then in step four we wouldn't have any output so this is a pretty complicated instruction for the model so let's see if it did it right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13bb7738",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-17T13:41:25.362170Z",
     "iopub.status.busy": "2024-03-17T13:41:25.361752Z",
     "iopub.status.idle": "2024-03-17T13:41:27.171832Z",
     "shell.execute_reply": "2024-03-17T13:41:27.169742Z"
    },
    "papermill": {
     "duration": 1.821062,
     "end_time": "2024-03-17T13:41:27.174505",
     "exception": false,
     "start_time": "2024-03-17T13:41:25.353443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:#### The user is comparing the prices of two specific products.\n",
      "Step 2:#### The products being compared are:\n",
      "- BlueWave Chromebook: Price $249.99\n",
      "- TechPro Desktop: Price $999.99\n",
      "Step 3:#### The user is assuming that the BlueWave Chromebook is more expensive than the TechPro Desktop.\n",
      "Step 4:#### The TechPro Desktop is actually more expensive than the BlueWave Chromebook by $750.\n",
      "Response to user:#### The BlueWave Chromebook is $750 cheaper than the TechPro Desktop.\n"
     ]
    }
   ],
   "source": [
    "user_message = f\"\"\"\n",
    "by how much is the BlueWave Chromebook more expensive \\\n",
    "than the TechPro Desktop\"\"\"\n",
    "\n",
    "messages =  [  \n",
    "{'role':'system', \n",
    " 'content': system_message},    \n",
    "{'role':'user', \n",
    " 'content': f\"{delimiter}{user_message}{delimiter}\"},  \n",
    "] \n",
    "\n",
    "response = get_completion_from_messages(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa9f6c1",
   "metadata": {
    "papermill": {
     "duration": 0.006958,
     "end_time": "2024-03-17T13:41:27.188610",
     "exception": false,
     "start_time": "2024-03-17T13:41:27.181652",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In step one the user asked a question about specific products. They're asking about the price difference between these two products the user assumes that the Blue Wave Chromebook is more expensive than the Techbook Pro and this assumption is incorrect.¬†\n",
    "\n",
    "The LLM reasoning is taking longer to think about the problem in the same way that a human would also take some time to reason about an answer to any given question.¬†\n",
    "The model performs better if it also has time to think and so the final response to the user is the Blue Wave Chromebook it's less expensive than the TechBook Pro which costs 999.99 while the Blue Wave Chromebook costs 249.99.¬†\n",
    "\n",
    "Let's take another example of a user message in which the user will ask **\"Do you sell TVs?\"** If you remember in our product list we've only listed different computers so let's see what the model says in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "69bf4de5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-17T13:41:27.205676Z",
     "iopub.status.busy": "2024-03-17T13:41:27.204179Z",
     "iopub.status.idle": "2024-03-17T13:41:28.396067Z",
     "shell.execute_reply": "2024-03-17T13:41:28.395001Z"
    },
    "papermill": {
     "duration": 1.204545,
     "end_time": "2024-03-17T13:41:28.400293",
     "exception": false,
     "start_time": "2024-03-17T13:41:27.195748",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:#### The user is asking a general question about whether the store sells TVs, not a specific product question. Therefore, the user is not asking about a specific product.\n"
     ]
    }
   ],
   "source": [
    "user_message = f\"\"\"\n",
    "do you sell tvs\"\"\"\n",
    "messages =  [  \n",
    "{'role':'system', \n",
    " 'content': system_message},    \n",
    "{'role':'user', \n",
    " 'content': f\"{delimiter}{user_message}{delimiter}\"},  \n",
    "] \n",
    "response = get_completion_from_messages(messages)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c732e5a",
   "metadata": {
    "papermill": {
     "duration": 0.008357,
     "end_time": "2024-03-17T13:41:28.416580",
     "exception": false,
     "start_time": "2024-03-17T13:41:28.408223",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In step one the user will ask if the store sells TVs but since TVs are not listed in the available products you can see the model then skip the response to the user step because it realizes that the intermediary steps are not necessary.\n",
    "\n",
    "I will say that we did ask for the output in this specific format so technically the model hasn't exactly followed our request again more advanced models will be better at doing that and so in this case our response to the user is \"I'm sorry but we do not sell TVs at the store\" and then it lists the available products."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a1eddd",
   "metadata": {
    "papermill": {
     "duration": 0.007795,
     "end_time": "2024-03-17T13:41:28.431522",
     "exception": false,
     "start_time": "2024-03-17T13:41:28.423727",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"4\"></a>\n",
    "# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 4. Inner Monologue Removal </b></div>\n",
    "\n",
    "\n",
    "If  we do not want to show the earlier parts of the response to the user we can just cut the string at the last occurrence of this delimiter token or string of four hashtags and then only print the final part of the model output.¬†\n",
    "\n",
    "Here is the code to get only the final part of this string. We will use a try accept block to gracefully handle errors in case the model has some unpredictable output and doesn't use these characters. We are going to split the string at the delimiter string because we just want to get the last item in the output list then we're going to strip any white space because as you can see there might be white space after the characters then we're going to catch any errors and have a fallback response which is \"**Sorry, I'm having trouble right now please try asking another question\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0e6666b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-17T13:41:28.447236Z",
     "iopub.status.busy": "2024-03-17T13:41:28.446828Z",
     "iopub.status.idle": "2024-03-17T13:41:28.453120Z",
     "shell.execute_reply": "2024-03-17T13:41:28.452055Z"
    },
    "papermill": {
     "duration": 0.017173,
     "end_time": "2024-03-17T13:41:28.455745",
     "exception": false,
     "start_time": "2024-03-17T13:41:28.438572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user is asking a general question about whether the store sells TVs, not a specific product question. Therefore, the user is not asking about a specific product.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    final_response = response.split(delimiter)[-1].strip()\n",
    "except Exception as e:\n",
    "    final_response = \"Sorry, I'm having trouble right now, please try asking another question.\"\n",
    "    \n",
    "print(final_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b023e616",
   "metadata": {
    "papermill": {
     "duration": 0.006869,
     "end_time": "2024-03-17T13:41:28.469735",
     "exception": false,
     "start_time": "2024-03-17T13:41:28.462866",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "If we were to integrate this prompt into an application, it's worth noting that this prompt is convoluted for the task at hand. You may find that you don't require all of these intermediary steps.¬†\n",
    "I encourage you to explore alternative approaches to achieve the same goal more efficiently. Generally, finding the optimal balance in prompt complexity necessitates some experimentation. It's beneficial to try out several variations before settling on one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07013a6b",
   "metadata": {
    "papermill": {
     "duration": 0.006962,
     "end_time": "2024-03-17T13:41:28.483767",
     "exception": false,
     "start_time": "2024-03-17T13:41:28.476805",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"box-shadow: rgba(240, 46, 170, 0.4) -5px 5px inset, rgba(240, 46, 170, 0.3) -10px 10px inset, rgba(240, 46, 170, 0.2) -15px 15px inset, rgba(240, 46, 170, 0.1) -20px 20px inset, rgba(240, 46, 170, 0.05) -25px 25px inset; padding:20px; font-size:30px; font-family: consolas; display:fill; border-radius:15px; color: rgba(240, 46, 170, 0.7)\"> <b> ‡ºº‚Å† ‚Å†„Å§‚Å† ‚Å†‚óï‚Å†‚Äø‚Å†‚óï‚Å† ‚Å†‡ºΩ‚Å†„Å§ Thank You!</b></div>\n",
    "\n",
    "<p style=\"font-family:verdana; color:rgb(34, 34, 34); font-family: consolas; font-size: 16px;\"> üíå Thank you for taking the time to read through my notebook. I hope you found it interesting and informative. If you have any feedback or suggestions for improvement, please don't hesitate to let me know in the comments. <br><br> üöÄ If you liked this notebook, please consider upvoting it so that others can discover it too. Your support means a lot to me, and it helps to motivate me to create more content in the future. <br><br> ‚ù§Ô∏è Once again, thank you for your support, and I hope to see you again soon!</p>"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30664,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 24.778624,
   "end_time": "2024-03-17T13:41:29.011079",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-17T13:41:04.232455",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
