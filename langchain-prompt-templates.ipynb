{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6be8c06",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.012816,
     "end_time": "2024-04-14T13:28:10.568201",
     "exception": false,
     "start_time": "2024-04-14T13:28:10.555385",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <center style=\"font-family: consolas; font-size: 32px; font-weight: bold;\">  Hands-On LangChain for LLM Applications Development: Prompt Templates </center>\n",
    "\n",
    "# <center style=\"font-family: consolas; font-size: 25px; font-weight: bold;\">  Understanding LangChain Prompt Templates   </center>\n",
    "***\n",
    "\n",
    "By prompting an LLM or large language model, it is possible to develop complex AI applications much faster than ever before. However, an application can require prompting an LLM multiple times and parsing its output, so a lot of glue code must be written.\n",
    "\n",
    "LangChain makes this development process much easier by using an easy set of abstractions to do this type of operation and by providing prompt templates. In this notebook, we will cover prompt templates, why it is important, and how to use them effectively, explained with practical examples.\n",
    "\n",
    "\n",
    "#### <a id=\"top\"></a>\n",
    "# <div style=\"box-shadow: rgb(60, 121, 245) 0px 0px 0px 3px inset, rgb(255, 255, 255) 10px -10px 0px -3px, rgb(31, 193, 27) 10px -10px, rgb(255, 255, 255) 20px -20px 0px -3px, rgb(255, 217, 19) 20px -20px, rgb(255, 255, 255) 30px -30px 0px -3px, rgb(255, 156, 85) 30px -30px, rgb(255, 255, 255) 40px -40px 0px -3px, rgb(255, 85, 85) 40px -40px; padding:20px; margin-right: 40px; font-size:30px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(60, 121, 245);\"><b>Table of contents</b></div>\n",
    "\n",
    "<div style=\"background-color: rgba(60, 121, 245, 0.03); padding:30px; font-size:15px; font-family: consolas;\">\n",
    "<ul>\n",
    "    <li><a href=\"#1\" target=\"_self\" rel=\" noreferrer nofollow\">1. Setting Up Working Environment & Getting Started </a> </li>\n",
    "    <li><a href=\"#2\" target=\"_self\" rel=\" noreferrer nofollow\">2. Prompt Template using LangChain </a></li>\n",
    "    <li><a href=\"#3\" target=\"_self\" rel=\" noreferrer nofollow\">3. Why do We Need LangChain Prompt Templates? </a></li> \n",
    "</ul>\n",
    "</div>\n",
    "\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b20888",
   "metadata": {
    "papermill": {
     "duration": 0.01315,
     "end_time": "2024-04-14T13:28:10.594457",
     "exception": false,
     "start_time": "2024-04-14T13:28:10.581307",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 1. Setting Up Working Environment & Getting Started </b></div>\n",
    "\n",
    "\n",
    "To get started we are going to import OS, import OpenAI, and load my OpenAI secret key. If you’re running this locally and don’t have OpenAI installed yet, you might need to run pip to install OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70124345",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T13:28:10.622130Z",
     "iopub.status.busy": "2024-04-14T13:28:10.621719Z",
     "iopub.status.idle": "2024-04-14T13:28:28.171471Z",
     "shell.execute_reply": "2024-04-14T13:28:28.169823Z"
    },
    "papermill": {
     "duration": 17.567035,
     "end_time": "2024-04-14T13:28:28.174427",
     "exception": false,
     "start_time": "2024-04-14T13:28:10.607392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai\r\n",
      "  Downloading openai-1.17.1-py3-none-any.whl.metadata (21 kB)\r\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/conda/lib/python3.10/site-packages (from openai) (4.2.0)\r\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai) (1.9.0)\r\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from openai) (0.27.0)\r\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from openai) (2.5.3)\r\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai) (1.3.0)\r\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.10/site-packages (from openai) (4.66.1)\r\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /opt/conda/lib/python3.10/site-packages (from openai) (4.9.0)\r\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (3.6)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai) (1.2.0)\r\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (2024.2.2)\r\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1.9.0->openai) (2.14.6)\r\n",
      "Downloading openai-1.17.1-py3-none-any.whl (268 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.3/268.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: openai\r\n",
      "Successfully installed openai-1.17.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b0bc4e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T13:28:28.206256Z",
     "iopub.status.busy": "2024-04-14T13:28:28.204915Z",
     "iopub.status.idle": "2024-04-14T13:28:29.298102Z",
     "shell.execute_reply": "2024-04-14T13:28:29.296836Z"
    },
    "papermill": {
     "duration": 1.113742,
     "end_time": "2024-04-14T13:28:29.301661",
     "exception": false,
     "start_time": "2024-04-14T13:28:28.187919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import openai\n",
    "import os\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "openai.api_key = user_secrets.get_secret(\"openai_api\")\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=openai.api_key,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f64ecf",
   "metadata": {
    "papermill": {
     "duration": 0.013618,
     "end_time": "2024-04-14T13:28:29.329278",
     "exception": false,
     "start_time": "2024-04-14T13:28:29.315660",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "After that, we will define the LLM model that we will use which will be gpt-3.5-turbo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ceb6d6ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T13:28:29.359928Z",
     "iopub.status.busy": "2024-04-14T13:28:29.359352Z",
     "iopub.status.idle": "2024-04-14T13:28:29.365078Z",
     "shell.execute_reply": "2024-04-14T13:28:29.363797Z"
    },
    "papermill": {
     "duration": 0.024765,
     "end_time": "2024-04-14T13:28:29.367988",
     "exception": false,
     "start_time": "2024-04-14T13:28:29.343223",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_model = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fa1c24",
   "metadata": {
    "papermill": {
     "duration": 0.013824,
     "end_time": "2024-04-14T13:28:29.396684",
     "exception": false,
     "start_time": "2024-04-14T13:28:29.382860",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next, we will define a helper function that will take the input prompt and return the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40cf6882",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T13:28:29.428215Z",
     "iopub.status.busy": "2024-04-14T13:28:29.426446Z",
     "iopub.status.idle": "2024-04-14T13:28:29.434569Z",
     "shell.execute_reply": "2024-04-14T13:28:29.433555Z"
    },
    "papermill": {
     "duration": 0.026105,
     "end_time": "2024-04-14T13:28:29.437088",
     "exception": false,
     "start_time": "2024-04-14T13:28:29.410983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_completion(prompt, \n",
    "                   llm_model=llm_model, \n",
    "                   temperature=0, \n",
    "                   max_tokens=500):\n",
    "\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=llm_model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, \n",
    "        max_tokens=max_tokens, \n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b228c6b3",
   "metadata": {
    "papermill": {
     "duration": 0.012952,
     "end_time": "2024-04-14T13:28:29.463552",
     "exception": false,
     "start_time": "2024-04-14T13:28:29.450600",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let’s start with an example where you get an email from a customer in a language other than formal English. To make sure the example is understandable, the other language we will use is the English pirate language:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8de7cdc8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T13:28:29.494198Z",
     "iopub.status.busy": "2024-04-14T13:28:29.493670Z",
     "iopub.status.idle": "2024-04-14T13:28:29.500355Z",
     "shell.execute_reply": "2024-04-14T13:28:29.499059Z"
    },
    "papermill": {
     "duration": 0.025824,
     "end_time": "2024-04-14T13:28:29.503422",
     "exception": false,
     "start_time": "2024-04-14T13:28:29.477598",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse,\\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feda3295",
   "metadata": {
    "papermill": {
     "duration": 0.013538,
     "end_time": "2024-04-14T13:28:29.531397",
     "exception": false,
     "start_time": "2024-04-14T13:28:29.517859",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We will ask the LLM to translate the text to formal English in a calm and respectful tone. I will set the style to American English in a calm and respectful tone. I will specify the prompt using an f-string with the instructions, translate the text that is delimited by triple backticks into style, and then plug in these two styles. This generates a prompt that says translate the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0e65e1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T13:28:29.561936Z",
     "iopub.status.busy": "2024-04-14T13:28:29.561510Z",
     "iopub.status.idle": "2024-04-14T13:28:29.568493Z",
     "shell.execute_reply": "2024-04-14T13:28:29.567070Z"
    },
    "papermill": {
     "duration": 0.025826,
     "end_time": "2024-04-14T13:28:29.571544",
     "exception": false,
     "start_time": "2024-04-14T13:28:29.545718",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks \n",
      "into a style that is American English in a calm and respectful tone\n",
      ".\n",
      "text: ```\n",
      "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse,the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \n",
    "into a style that is {style}.\n",
    "text: ```{customer_email}```\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8f73aa",
   "metadata": {
    "papermill": {
     "duration": 0.014058,
     "end_time": "2024-04-14T13:28:29.600442",
     "exception": false,
     "start_time": "2024-04-14T13:28:29.586384",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let’s see what the response is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc2d4830",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T13:28:29.630206Z",
     "iopub.status.busy": "2024-04-14T13:28:29.629759Z",
     "iopub.status.idle": "2024-04-14T13:28:30.633839Z",
     "shell.execute_reply": "2024-04-14T13:28:30.632328Z"
    },
    "papermill": {
     "duration": 1.022324,
     "end_time": "2024-04-14T13:28:30.636646",
     "exception": false,
     "start_time": "2024-04-14T13:28:29.614322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I am really frustrated that my blender lid flew off and splattered my kitchen walls with smoothie! And to make matters worse, the warranty doesn't cover the cost of cleaning up my kitchen. I need your help right now, friend.\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = get_completion(prompt)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20ddd534",
   "metadata": {
    "papermill": {
     "duration": 0.014079,
     "end_time": "2024-04-14T13:28:30.665815",
     "exception": false,
     "start_time": "2024-04-14T13:28:30.651736",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "That sounds very nice and calm. Therefore if you have different customers writing reviews in different languages, not just English pirates, but French, German, Japanese, and so on, you can imagine having to generate a whole sequence of prompts to generate such translations. Let’s look at how we can do this in a more convenient, way using **LangChain**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666eba7f",
   "metadata": {
    "papermill": {
     "duration": 0.014306,
     "end_time": "2024-04-14T13:28:30.693878",
     "exception": false,
     "start_time": "2024-04-14T13:28:30.679572",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"2\"></a>\n",
    "# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 2. Prompt Template using LangChain </b></div>\n",
    "\n",
    "\n",
    "\n",
    "Let's start with importing chat OpenAI. This is LangChain’s abstraction for the chatGPT API endpoint. I will set the temperature parameter to be equal to zero to make the output a little bit less random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71582931",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T13:28:30.724526Z",
     "iopub.status.busy": "2024-04-14T13:28:30.724134Z",
     "iopub.status.idle": "2024-04-14T13:28:50.562735Z",
     "shell.execute_reply": "2024-04-14T13:28:50.561444Z"
    },
    "papermill": {
     "duration": 19.857929,
     "end_time": "2024-04-14T13:28:50.565532",
     "exception": false,
     "start_time": "2024-04-14T13:28:30.707603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.10/site-packages (23.3.2)\r\n",
      "Collecting install\r\n",
      "  Downloading install-1.3.5-py3-none-any.whl.metadata (925 bytes)\r\n",
      "Collecting langchain-community\r\n",
      "  Downloading langchain_community-0.0.32-py3-none-any.whl.metadata (8.5 kB)\r\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (6.0.1)\r\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (2.0.25)\r\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (3.9.1)\r\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (0.6.4)\r\n",
      "Collecting langchain-core<0.2.0,>=0.1.41 (from langchain-community)\r\n",
      "  Downloading langchain_core-0.1.42-py3-none-any.whl.metadata (5.9 kB)\r\n",
      "Collecting langsmith<0.2.0,>=0.1.0 (from langchain-community)\r\n",
      "  Downloading langsmith-0.1.47-py3-none-any.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: numpy<2,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (1.26.4)\r\n",
      "Requirement already satisfied: requests<3,>=2 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (2.31.0)\r\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain-community) (8.2.3)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (23.2.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.0.4)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.9.3)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.1)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.1)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (4.0.3)\r\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.21.1)\r\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\r\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.41->langchain-community) (1.33)\r\n",
      "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.41->langchain-community)\r\n",
      "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\r\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.2.0,>=0.1.41->langchain-community) (2.5.3)\r\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.0->langchain-community)\r\n",
      "  Downloading orjson-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2->langchain-community) (2024.2.2)\r\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (4.9.0)\r\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.0.3)\r\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.41->langchain-community) (2.4)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.41->langchain-community) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.41->langchain-community) (2.14.6)\r\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\r\n",
      "Downloading install-1.3.5-py3-none-any.whl (3.2 kB)\r\n",
      "Downloading langchain_community-0.0.32-py3-none-any.whl (1.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading langchain_core-0.1.42-py3-none-any.whl (287 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.5/287.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading langsmith-0.1.47-py3-none-any.whl (113 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.0/113.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading orjson-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.8/144.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: packaging, orjson, install, langsmith, langchain-core, langchain-community\r\n",
      "  Attempting uninstall: packaging\r\n",
      "    Found existing installation: packaging 21.3\r\n",
      "    Uninstalling packaging-21.3:\r\n",
      "      Successfully uninstalled packaging-21.3\r\n",
      "  Attempting uninstall: orjson\r\n",
      "    Found existing installation: orjson 3.9.10\r\n",
      "    Uninstalling orjson-3.9.10:\r\n",
      "      Successfully uninstalled orjson-3.9.10\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "keras-cv 0.8.2 requires keras-core, which is not installed.\r\n",
      "keras-nlp 0.8.2 requires keras-core, which is not installed.\r\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\r\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.2 which is incompatible.\r\n",
      "google-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\r\n",
      "jupyterlab 4.1.5 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\n",
      "jupyterlab-lsp 5.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\r\n",
      "libpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "momepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "osmnx 1.9.2 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "spopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\r\n",
      "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.1.1 which is incompatible.\r\n",
      "ydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed install-1.3.5 langchain-community-0.0.32 langchain-core-0.1.42 langsmith-0.1.47 orjson-3.10.0 packaging-23.2\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "484de366",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T13:28:50.601074Z",
     "iopub.status.busy": "2024-04-14T13:28:50.600550Z",
     "iopub.status.idle": "2024-04-14T13:28:51.290973Z",
     "shell.execute_reply": "2024-04-14T13:28:51.289816Z"
    },
    "papermill": {
     "duration": 0.712378,
     "end_time": "2024-04-14T13:28:51.294247",
     "exception": false,
     "start_time": "2024-04-14T13:28:50.581869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.openai.ChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.chat_models import ChatOpenAI\n",
    "\n",
    "# To control the randomness and creativity of the generated\n",
    "# text by an LLM, use temperature = 0.0\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.0, model=llm_model, openai_api_key=openai.api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b951e32",
   "metadata": {
    "papermill": {
     "duration": 0.017162,
     "end_time": "2024-04-14T13:28:51.328432",
     "exception": false,
     "start_time": "2024-04-14T13:28:51.311270",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We will define the template string as follows. Translate the text delimited by triple vectors into a style that is style, and then here’s the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ae1b0492",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T13:28:51.364747Z",
     "iopub.status.busy": "2024-04-14T13:28:51.364332Z",
     "iopub.status.idle": "2024-04-14T13:28:51.369307Z",
     "shell.execute_reply": "2024-04-14T13:28:51.368056Z"
    },
    "papermill": {
     "duration": 0.026177,
     "end_time": "2024-04-14T13:28:51.371646",
     "exception": false,
     "start_time": "2024-04-14T13:28:51.345469",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "template_string = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830f75ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T12:37:45.308614Z",
     "iopub.status.busy": "2024-04-14T12:37:45.308162Z",
     "iopub.status.idle": "2024-04-14T12:37:45.317049Z",
     "shell.execute_reply": "2024-04-14T12:37:45.315750Z",
     "shell.execute_reply.started": "2024-04-14T12:37:45.308580Z"
    },
    "papermill": {
     "duration": 0.017432,
     "end_time": "2024-04-14T13:28:51.406168",
     "exception": false,
     "start_time": "2024-04-14T13:28:51.388736",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To repeatedly reuse this template, we have to import LangChain’s chat prompt template, and then, let me create a prompt template using that template string that we just wrote above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "440a3fb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T13:28:51.442305Z",
     "iopub.status.busy": "2024-04-14T13:28:51.441852Z",
     "iopub.status.idle": "2024-04-14T13:28:51.540936Z",
     "shell.execute_reply": "2024-04-14T13:28:51.539943Z"
    },
    "papermill": {
     "duration": 0.120846,
     "end_time": "2024-04-14T13:28:51.544024",
     "exception": false,
     "start_time": "2024-04-14T13:28:51.423178",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c9c8ee",
   "metadata": {
    "papermill": {
     "duration": 0.016698,
     "end_time": "2024-04-14T13:28:51.578863",
     "exception": false,
     "start_time": "2024-04-14T13:28:51.562165",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "From the prompt template, you can extract the original prompt, and it realizes that this prompt has two input variables, the style, and the text, shown here with the curly braces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b8195acd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T13:28:51.617303Z",
     "iopub.status.busy": "2024-04-14T13:28:51.616003Z",
     "iopub.status.idle": "2024-04-14T13:28:51.624845Z",
     "shell.execute_reply": "2024-04-14T13:28:51.623684Z"
    },
    "papermill": {
     "duration": 0.030186,
     "end_time": "2024-04-14T13:28:51.627347",
     "exception": false,
     "start_time": "2024-04-14T13:28:51.597161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['style', 'text'], template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faa866df",
   "metadata": {
    "papermill": {
     "duration": 0.016528,
     "end_time": "2024-04-14T13:28:51.661119",
     "exception": false,
     "start_time": "2024-04-14T13:28:51.644591",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We can also print the input variables out, and you can see that it realizes it has two input variables the style and text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cb69da7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T13:28:51.698072Z",
     "iopub.status.busy": "2024-04-14T13:28:51.696697Z",
     "iopub.status.idle": "2024-04-14T13:28:51.704888Z",
     "shell.execute_reply": "2024-04-14T13:28:51.703907Z"
    },
    "papermill": {
     "duration": 0.029279,
     "end_time": "2024-04-14T13:28:51.707396",
     "exception": false,
     "start_time": "2024-04-14T13:28:51.678117",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['style', 'text']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template.messages[0].prompt.input_variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea93dbec",
   "metadata": {
    "papermill": {
     "duration": 0.016664,
     "end_time": "2024-04-14T13:28:51.743645",
     "exception": false,
     "start_time": "2024-04-14T13:28:51.726981",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now, let’s specify the style. This is a style that I want the customer message to be translated to, so I’m going to call this customer style, and here’s my same customer email as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93d781e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T13:28:51.845164Z",
     "iopub.status.busy": "2024-04-14T13:28:51.844367Z",
     "iopub.status.idle": "2024-04-14T13:28:51.849856Z",
     "shell.execute_reply": "2024-04-14T13:28:51.848685Z"
    },
    "papermill": {
     "duration": 0.091941,
     "end_time": "2024-04-14T13:28:51.852454",
     "exception": false,
     "start_time": "2024-04-14T13:28:51.760513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "customer_style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\"\n",
    "\n",
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e99abf",
   "metadata": {
    "papermill": {
     "duration": 0.016515,
     "end_time": "2024-04-14T13:28:51.885811",
     "exception": false,
     "start_time": "2024-04-14T13:28:51.869296",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "If I create customer messages, this will generate the prompt and will pass this large language model to get a response.\n",
    "\n",
    "If you want to look at the types, the customer message is a list, and if you look at the first element of the list, this is more or less the prompt that you would expect this to be creating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c65a075f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T13:28:51.921729Z",
     "iopub.status.busy": "2024-04-14T13:28:51.921296Z",
     "iopub.status.idle": "2024-04-14T13:28:51.927835Z",
     "shell.execute_reply": "2024-04-14T13:28:51.926353Z"
    },
    "papermill": {
     "duration": 0.028438,
     "end_time": "2024-04-14T13:28:51.930976",
     "exception": false,
     "start_time": "2024-04-14T13:28:51.902538",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'langchain_core.messages.human.HumanMessage'>\n"
     ]
    }
   ],
   "source": [
    "customer_messages = prompt_template.format_messages(\n",
    "                    style=customer_style,\n",
    "                    text=customer_email)\n",
    "\n",
    "print(type(customer_messages))\n",
    "print(type(customer_messages[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880e1063",
   "metadata": {
    "papermill": {
     "duration": 0.016518,
     "end_time": "2024-04-14T13:28:51.964833",
     "exception": false,
     "start_time": "2024-04-14T13:28:51.948315",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Lastly, let’s pass this prompt to the LLM, so I’m going to call chat, which we had set earlier, as a reference to the OpenAI chatGPT endpoint, and, if we print out the customer responses content, then, it gives you back this text translated from English pirate to polite American English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "44e39588",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T13:28:52.002402Z",
     "iopub.status.busy": "2024-04-14T13:28:52.001568Z",
     "iopub.status.idle": "2024-04-14T13:28:53.162034Z",
     "shell.execute_reply": "2024-04-14T13:28:53.160830Z"
    },
    "papermill": {
     "duration": 1.182205,
     "end_time": "2024-04-14T13:28:53.164735",
     "exception": false,
     "start_time": "2024-04-14T13:28:51.982530",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `__call__` was deprecated in LangChain 0.1.7 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oh man, I'm really frustrated that my blender lid flew off and made a mess of my kitchen walls with smoothie! And on top of that, the warranty doesn't cover the cost of cleaning up my kitchen. I could really use your help right now, friend.\n"
     ]
    }
   ],
   "source": [
    "# Call the LLM to translate to the style of the customer message\n",
    "customer_response = chat(customer_messages)\n",
    "\n",
    "print(customer_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a020e73",
   "metadata": {
    "papermill": {
     "duration": 0.01685,
     "end_time": "2024-04-14T13:28:53.198973",
     "exception": false,
     "start_time": "2024-04-14T13:28:53.182123",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Of course, you can imagine other use cases where the customer emails are in other languages and this too can be used to translate the messages for an English-speaking to understand and reply to.\n",
    "\n",
    "So let’s say, an English-speaking customer service agent writes this and says,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df56f6c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T13:28:53.237103Z",
     "iopub.status.busy": "2024-04-14T13:28:53.236504Z",
     "iopub.status.idle": "2024-04-14T13:28:53.241634Z",
     "shell.execute_reply": "2024-04-14T13:28:53.240683Z"
    },
    "papermill": {
     "duration": 0.02798,
     "end_time": "2024-04-14T13:28:53.244113",
     "exception": false,
     "start_time": "2024-04-14T13:28:53.216133",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "service_reply = \"\"\"Hey there customer, \\\n",
    "the warranty does not cover \\\n",
    "cleaning expenses for your kitchen \\\n",
    "because it's your fault that \\\n",
    "you misused your blender \\\n",
    "by forgetting to put the lid on before \\\n",
    "starting the blender. \\\n",
    "Tough luck! See ya!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f7c648",
   "metadata": {
    "papermill": {
     "duration": 0.016809,
     "end_time": "2024-04-14T13:28:53.278546",
     "exception": false,
     "start_time": "2024-04-14T13:28:53.261737",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "But let’s say this is what a customer service agent wants. We are going to specify that the service message is going to be translated to this pirate style. So we want it to be in a polite tone that speaks in English pirate. And because we previously created that prompt template, the cool thing is, that we can now reuse that prompt template and specify that the output style we want is this service style pirate and the text is this service reply.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a22be118",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T13:28:53.315920Z",
     "iopub.status.busy": "2024-04-14T13:28:53.315362Z",
     "iopub.status.idle": "2024-04-14T13:28:53.321676Z",
     "shell.execute_reply": "2024-04-14T13:28:53.320322Z"
    },
    "papermill": {
     "duration": 0.027783,
     "end_time": "2024-04-14T13:28:53.324090",
     "exception": false,
     "start_time": "2024-04-14T13:28:53.296307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "service_style_pirate = \"\"\"\\\n",
    "a polite tone \\\n",
    "that speaks in English Pirate\\\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82517ec9",
   "metadata": {
    "papermill": {
     "duration": 0.01679,
     "end_time": "2024-04-14T13:28:53.358338",
     "exception": false,
     "start_time": "2024-04-14T13:28:53.341548",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "And if we do that, that’s the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca7f08a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T13:28:53.396717Z",
     "iopub.status.busy": "2024-04-14T13:28:53.395655Z",
     "iopub.status.idle": "2024-04-14T13:28:53.403468Z",
     "shell.execute_reply": "2024-04-14T13:28:53.401987Z"
    },
    "papermill": {
     "duration": 0.030665,
     "end_time": "2024-04-14T13:28:53.406223",
     "exception": false,
     "start_time": "2024-04-14T13:28:53.375558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks into a style that is a polite tone that speaks in English Pirate. text: ```Hey there customer, the warranty does not cover cleaning expenses for your kitchen because it's your fault that you misused your blender by forgetting to put the lid on before starting the blender. Tough luck! See ya!\n",
      "```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "service_messages = prompt_template.format_messages(\n",
    "    style=service_style_pirate,\n",
    "    text=service_reply)\n",
    "\n",
    "print(service_messages[0].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad523078",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T13:11:17.655138Z",
     "iopub.status.busy": "2024-04-14T13:11:17.654670Z",
     "iopub.status.idle": "2024-04-14T13:11:17.662657Z",
     "shell.execute_reply": "2024-04-14T13:11:17.661146Z",
     "shell.execute_reply.started": "2024-04-14T13:11:17.655105Z"
    },
    "papermill": {
     "duration": 0.017821,
     "end_time": "2024-04-14T13:28:53.442264",
     "exception": false,
     "start_time": "2024-04-14T13:28:53.424443",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "And if we prompt, ChatGPT, this is the response it gives us back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d217d102",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T13:28:53.480422Z",
     "iopub.status.busy": "2024-04-14T13:28:53.479857Z",
     "iopub.status.idle": "2024-04-14T13:28:54.829927Z",
     "shell.execute_reply": "2024-04-14T13:28:54.828665Z"
    },
    "papermill": {
     "duration": 1.374069,
     "end_time": "2024-04-14T13:28:54.833939",
     "exception": false,
     "start_time": "2024-04-14T13:28:53.459870",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ahoy there, valued customer! Regrettably, the warranty be not coverin' the cost o' cleanin' yer galley due to yer own negligence. Ye see, 'twas yer own doin' fer forgettin' to secure the lid afore startin' the blender. 'Tis a tough break, matey! Fare thee well!\n"
     ]
    }
   ],
   "source": [
    "service_response = chat(service_messages)\n",
    "print(service_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703078f4",
   "metadata": {
    "papermill": {
     "duration": 0.017503,
     "end_time": "2024-04-14T13:28:54.870469",
     "exception": false,
     "start_time": "2024-04-14T13:28:54.852966",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"3\"></a>\n",
    "# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 3. Why do We Need LangChain Prompt Templates? </b></div>\n",
    "\n",
    "\n",
    "\n",
    "We are using prompt templates instead of just an f-string prompt because as you build sophisticated applications, prompts can be quite long and detailed. Prompt templates are a useful abstraction to help you reuse good prompts when you can.\n",
    "\n",
    "This is an example of a relatively long prompt to decide if a student’s solution is correct or not. And a prompt like this can be quite long, in which you can ask the LLM to first solve the problem, and then have the output in a certain format, and the output in a certain format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4278c21",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-14T13:28:54.908347Z",
     "iopub.status.busy": "2024-04-14T13:28:54.907872Z",
     "iopub.status.idle": "2024-04-14T13:28:54.914447Z",
     "shell.execute_reply": "2024-04-14T13:28:54.913267Z"
    },
    "papermill": {
     "duration": 0.02851,
     "end_time": "2024-04-14T13:28:54.916865",
     "exception": false,
     "start_time": "2024-04-14T13:28:54.888355",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Determine if the student's solution is correct or not.\n",
    "\n",
    "Question:\n",
    "I'm building a solar power installation and I need \\\n",
    " help working out the financials. \n",
    "- Land costs $100 / square foot\n",
    "- I can buy solar panels for $250 / square foot\n",
    "- I negotiated a contract for maintenance that will cost \\ \n",
    "me a flat $100k per year, and an additional $10 / square \\\n",
    "foot\n",
    "What is the total cost for the first year of operations \n",
    "as a function of the number of square feet.\n",
    "\n",
    "Student's Solution:\n",
    "Let x be the size of the installation in square feet.\n",
    "Costs:\n",
    "1. Land cost: 100x\n",
    "2. Solar panel cost: 250x\n",
    "3. Maintenance cost: 100,000 + 100x\n",
    "Total cost: 100x + 250x + 100,000 + 100x = 450x + 100,000\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f47e4e",
   "metadata": {
    "papermill": {
     "duration": 0.017617,
     "end_time": "2024-04-14T13:28:54.952299",
     "exception": false,
     "start_time": "2024-04-14T13:28:54.934682",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "LangChain prompt makes it easier to reuse a prompt like this. Also, as you will see in the next articles LangChain provides prompts for some common operations, such as summarization, question answering, connecting to SQL databases, or connecting to different APIs. So by using some of LangChain’s built-in prompts, you can quickly get an application working without needing to, engineer your prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cdc3a8",
   "metadata": {
    "papermill": {
     "duration": 0.017807,
     "end_time": "2024-04-14T13:28:54.987748",
     "exception": false,
     "start_time": "2024-04-14T13:28:54.969941",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <div style=\"box-shadow: rgba(240, 46, 170, 0.4) -5px 5px inset, rgba(240, 46, 170, 0.3) -10px 10px inset, rgba(240, 46, 170, 0.2) -15px 15px inset, rgba(240, 46, 170, 0.1) -20px 20px inset, rgba(240, 46, 170, 0.05) -25px 25px inset; padding:20px; font-size:30px; font-family: consolas; display:fill; border-radius:15px; color: rgba(240, 46, 170, 0.7)\"> <b> ༼⁠ ⁠つ⁠ ⁠◕⁠‿⁠◕⁠ ⁠༽⁠つ Thank You!</b></div>\n",
    "\n",
    "<p style=\"font-family:verdana; color:rgb(34, 34, 34); font-family: consolas; font-size: 16px;\"> 💌 Thank you for taking the time to read through my notebook. I hope you found it interesting and informative. If you have any feedback or suggestions for improvement, please don't hesitate to let me know in the comments. <br><br> 🚀 If you liked this notebook, please consider upvoting it so that others can discover it too. Your support means a lot to me, and it helps to motivate me to create more content in the future. <br><br> ❤️ Once again, thank you for your support, and I hope to see you again soon!</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef35c653",
   "metadata": {
    "papermill": {
     "duration": 0.01843,
     "end_time": "2024-04-14T13:28:55.023622",
     "exception": false,
     "start_time": "2024-04-14T13:28:55.005192",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30684,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 48.611658,
   "end_time": "2024-04-14T13:28:55.966463",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-04-14T13:28:07.354805",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
